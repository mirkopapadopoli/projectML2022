```{r}
#import library
library(e1071)
library(ggplot2)
library(dplyr)
library(Hmisc)
library(corrplot)
library(caret)
library(plyr)
library(rpart)
library(rattle)
library(ROCR)
library(FactoMineR)
library(factoextra)
library(C50)
library(pROC)
```

```{r}
#import csv
waterquality = read.csv("waterQuality.csv")
```

```{r}
#preprocessing se duplicati e nulli
sum(duplicated(waterquality))
sum(is.na(waterquality))
```

```{r}
 
waterquality_pre <- waterquality[!(waterquality$is_safe =="#NUM!"),]
```

```{r}
#conversione variabili
waterquality_pre$ammonia = as.numeric(waterquality_pre$ammonia)
waterquality_pre$is_safe = factor(waterquality_pre$is_safe)
```

```{r}
head(waterquality_pre)
```

```{r}
sapply(waterquality_pre, class)
```

```{r}
describe(waterquality_pre)
```

```{r}
dim(waterquality_pre)
```

```{r}
ggplot(data.frame(waterquality_pre$is_safe), aes(x=waterquality_pre$is_safe)) +
  geom_bar()
```

```{r}
waterquality_pre %>%
  filter( uranium < max(uranium) ) %>%
  ggplot( aes(x=uranium)) +
    geom_density(fill="#69b3a2", color="#e9ecef")
```

```{r}
waterquality %>%
  filter( distance_from_home<max(distance_from_home) ) %>%
  ggplot( aes(x=distance_from_home)) +
    geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8)
```

```{r}
waterquality %>%
  filter( distance_from_last_transaction<max(distance_from_last_transaction) ) %>%
  ggplot( aes(x=distance_from_last_transaction)) +
    geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8)
```

```{r}
waterquality %>%
  filter( ratio_to_median_purchase_price<max(ratio_to_median_purchase_price) ) %>%
  ggplot( aes(x=ratio_to_median_purchase_price)) +
    geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8)
```

```{r}
ggplot(data.frame(waterquality$repeat_retailer), aes(x=waterquality$repeat_retailer)) +
  geom_bar()
```

```{r}
  ggplot(data.frame(waterquality$used_chip), aes(x=waterquality$used_chip)) +
  geom_bar()
```

```{r}
ggplot(data.frame(waterquality$used_pin_number), aes(x=waterquality$used_pin_number)) +
  geom_bar()
```

```{r}
ggplot(data.frame(waterquality$online_order), aes(x=waterquality$online_order)) +
  geom_bar()
```

```{r}
ggplot(data.frame(waterquality$fraud), aes(x=waterquality$fraud)) +
  geom_bar()
```

```{r}
#sistemare ggploot
sub <- subset(waterquality, select = c(distance_from_home, distance_from_last_transaction, ratio_to_median_purchase_price))

waterquality %>%
  filter( ratio_to_median_purchase_price < max(ratio_to_median_purchase_price) ) %>%
  ggplot(sub, aes(x=sub, y=len)) + 
  geom_boxplot()
```

```{r}
describe(waterquality)
```

```{r}
#camviare sizes
sizes <- c(35044,10000)
labels <- c("No Fraud","Fraud")

ggplot(data=NULL, aes(x=labels, y=sizes, fill=labels)) +
  geom_bar(stat="identity") +
  labs(title="What Percentage of Transactions Using Chip and Password are Fraud Transactions?", fill="") +
  theme(legend.position="none")
```

```{r}
#scatter matrix tra var continue
my_cols <- c("#00AFBB", "#E7B800", "#FC4E07")  
pairs(waterquality[,1:3], pch = 19,  cex = 0.5,
      col = my_cols[waterquality$fraud],
      lower.panel=NULL)
```

```{r}
#corr matrix
card.cor = cor(waterquality[,1:3])
corrplot(card.cor)
```

```{r}
#relazione se è frode usando torta
sub.fraud = waterquality[which(waterquality$fraud == 1),]
sub.nrow = nrow(sub.fraud)
for(i in 4:7){
  sub.zero = length(sub.fraud[which(sub.fraud[,i] == 0), i])
  sub.one = sub.nrow - sub.zero
  slices <- c(sub.zero, sub.one)
  lbls <- c("non c'e'", "c'e'")
  pct <- round(slices/sum(slices)*100)
  lbls <- paste(lbls, pct) # add percents to labels
  lbls <- paste(lbls,"%",sep="") # ad % to labels
  pie(slices,labels = lbls, col=rainbow(length(lbls)),
   main=colnames(waterquality)[i])
}
#se c'è stata una frode, sicuramente è perché non c'era il pin
```

```{r}
sub.nofraud = waterquality[which(waterquality$fraud == 0),]
sub.nonrow = nrow(sub.nofraud)
for(i in 4:7){
  sub.zero = length(sub.nofraud[which(sub.nofraud[,i] == 0), i])
  sub.one = sub.nonrow - sub.zero
  slices <- c(sub.zero, sub.one)
  lbls <- c("non c'e'", "c'e'")
  pct <- round(slices/sum(slices)*100)
  lbls <- paste(lbls, pct) # add percents to labels
  lbls <- paste(lbls,"%",sep="") # ad % to labels
  pie(slices,labels = lbls, col=rainbow(length(lbls)),
   main=colnames(waterquality)[i])
}
```

PARTE PCA

```{r}
res.pca <- PCA(waterquality_pre[,!names(waterquality_pre) %in% c("is_safe")], graph = FALSE)
eig.val <- get_eigenvalue(res.pca)
eig.val
```

```{r}
res.pca <- PCA(waterquality_pre[,!names(waterquality_pre) %in% c("is_safe")], graph = FALSE, ncp = 10)
```

```{r}
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
```

```{r}
var <- get_pca_var(res.pca)
```

```{r}
fviz_pca_var(res.pca, col.var = "black")
```

```{r}
ind <- get_pca_ind(res.pca)

```

```{r}
fviz_pca_ind(res.pca, col.ind = 'cos2', gradient.cols = c('#00AFBB', '#E7B800', '#FC4E07'),
repel = TRUE # Avoid text overlapping (slow if many points)
)
```

```{r}
split.data = function(data, p = 0.7, s = 1){
set.seed(s)
index = sample(1:dim(data)[1])
train = data[index[1:floor(dim(data)[1] * p)], ]
test = data[index[((ceiling(dim(data)[1] * p)) + 1):dim(data)[1]], ]
return(list(train=train, test=test)) } 
```

```{r}
dataframe_pca=as.data.frame(ind$coord)
dataframe_pca$is_safe = waterquality_pre$is_safe
```

PARTE DECISION TREE

```{r}
allset= split.data(dataframe_pca, s = 300)
trainset= allset$train
testset= allset$test
prop.table(table(trainset$is_safe))
```

```{r}
train_models = function(dataframe, grid, tipo, s = 44){

  set.seed(s)
  # Define the number of folds
  k = 5
  
  # Split the data into k folds
  folds = createFolds(dataframe$is_safe, k = k, returnTrain = TRUE)
  
  # Define an empty list to store the models
  models = list()
  
  # Loop through the grid of parameter values
  for (i in 1:nrow(grid)) {
      # Define an empty vector to store the accuracy
      accuracy = numeric()
      # Loop through the k-folds
      for (j in 1:k) {
          # Get the training and testing sets
          train_ind = folds[[j]]
          test_ind = -train_ind
          train_data = dataframe[train_ind, ]
          test_data = dataframe[test_ind, ]
        
          # Fit the model
          if(tipo == "rpart")
          {
            model = rpart(is_safe ~ ., 
                             data = train_data, 
                             method = "class",
                             control = rpart.control(
                               minsplit = grid[i, "minsplit"],
                               maxdepth = grid[i, "maxdepth"]))
          }
          else{
            model = svm(is_safe ~ ., 
                        data=train_data,
                        kernel = "radial", 
                        cost = grid[i, "C"],
                        scale=TRUE)
          }
        
          # Make predictions on the test data
          predictions = predict(model, test_data, type = "class")
        
          # Calculate the accuracy
          accuracy[j] = mean(predictions == test_data$is_safe)
      }
    
      # Store the model and its mean accuracy in the list
      models[[i]] = list(model = model, mean_accuracy = mean(accuracy))
  }
  
  # Find the model with the highest mean accuracy
  best_model = models[[which.max(sapply(models, function(x) x$mean_accuracy))]]
  
  # View the best model
  return(best_model$model)
}
```

```{r}
grid_tree = expand.grid(minsplit = c(2, 4), maxdepth = c(3, 5, 7, 9))
model.tree = train_models(trainset, grid_tree, "rpart", 42)
```

```{r}
model.tree
```

```{r}
fancyRpartPlot(model.tree)
```

```{r}
test.tree = testset[,!names(testset) %in% c("is_safe")]
testset$Prediction <- predict(model.tree, test.tree, type = "class")
```

```{r}
confusionmatrix.tree = table(testset$is_safe, testset$Prediction)
sum(diag(confusionmatrix.tree))/sum(confusionmatrix.tree)

```

```{r}
printcp(model.tree)
```

```{r}
plotcp(model.tree)
```

```{r}
model.treePR = prune(model.tree, cp=.017)
```

```{r}
fancyRpartPlot(model.treePR)
```

```{r}
testset$PredictionPR <- predict(model.treePR, test.tree, type = "class")
result.treePR = confusionMatrix(testset$PredictionPR, testset[,c("is_safe")], mode = "prec_recall")
result.treePR
```

PARTE SVM

```{r}
grid_svm = expand.grid(C = c(1))
svm.model = train_models(trainset, grid_svm, "svm", 42)
#svm.model = svm(is_safe ~ ., data=trainset, kernel='radial', cost=1, scale=TRUE) 
```

```{r}
print(svm.model)
```

```{r}
test.svm = testset[,!names(testset) %in% c("is_safe","Prediction","PredictionPR")]
```

```{r}
svm.pred = predict(svm.model, test.svm)

```

```{r}
result.svm = confusionMatrix(svm.pred, testset$is_safe, mode = "prec_recall")
result.svm
```

```{r}
#tuned = tune.svm(is_safe ~ ., data = trainset,kernel='radial',
#cost=c(10,0.1,1,0.01))
grid_svmT = expand.grid(C = c(0.1, 1, 10, 100))
svmT.model = train_models(trainset, grid_svmT, "svm", 42)
#svmT = tuned$best.model
```

```{r}
svmT.pred = predict(svmT.model, test.svm)
```

```{r}
result.svmT = confusionMatrix(svmT.pred, testset$is_safe, mode = "prec_recall")
result.svmT
```

PARTE ROC E AUC

```{r}
pred.treePR.prob = predict(prunedDecisionTree, test.tree, type = "prob")
pred.treePR.roc = pred.treePR.prob[, 2] 
```

```{r}
pred.rocr.treePR = prediction(pred.treePR.roc, testset$is_safe) 
perf.rocr.treePR = performance(pred.rocr.treePR, measure = "auc", x.measure = "cutoff")
perf.tpr.rocr.treePR = performance(pred.rocr.treePR, "tpr","fpr")
```

```{r}
plot(perf.tpr.rocr.treePR, colorize=T,main=paste("AUC:",(perf.rocr.treePR@y.values))) 
abline(a=0, b=1)
```

```{r}
tuned2 = tune.svm(is_safe ~ ., data = trainset,kernel='radial',
cost=c(10,0.1,1,0.01), prob = TRUE)
pred.svmT.prob = predict(tuned2$best.model, test.tree, probability=TRUE)
prob.svmT = attr(pred.svmT.prob, "probabilities")
pred.svmT.roc = prob.svmT[, 2] 
```

```{r}
pred.rocr.svmT = prediction(pred.svmT.roc, testset$is_safe  ) 
perf.rocr.svmT = performance(pred.rocr.svmT, measure = 'auc', x.measure = 'cutoff')
perf.tpr.rocr.svmT = performance(pred.rocr.svmT, "tpr","fpr") 

```

```{r}
plot(perf.tpr.rocr.svmT, colorize=T,main=paste("AUC:",(perf.rocr.svmT@y.values)))
abline(a=0, b=1)
```

```{r}
opt.cut = function(perf, pred){
 cut.ind = mapply(FUN=function(x, y, p){
 d = (x - 0)^2 + (y-1)^2
 ind = which(d == min(d))
 c(sensitivity = y[[ind]], specificity = 1-x[[ind]],
 cutoff = p[[ind]])
 }, perf@x.values, perf@y.values, pred@cutoffs)
}
```

```{r}
print(opt.cut(perf.tpr.rocr.svmT, pred.rocr.svmT))
```

```{r}
control = trainControl(method = "repeatedcv", number = 10,repeats = 3,
classProbs = TRUE, summaryFunction = twoClassSummary)
```

```{r}
comp_models_roc = function(trainset, meth, control, testset, color, added){
  model= train(make.names(is_safe) ~ ., data = trainset, method = meth, metric =
"ROC", trControl = control)
  probs = predict(model, testset[,! names(testset) %in% c("is_safe")], type = "prob")
  colnames(probs)[1] ="no"
  colnames(probs)[2] ="yes"
  ROC = roc(response = testset[,c("is_safe")], predictor = probs$yes,
  levels = levels(testset[,c("is_safe")]), direction = ">")
  return(ROC)
}
```

```{r}
control = trainControl(method = "repeatedcv", number = 10,repeats = 3,
classProbs = TRUE, summaryFunction = twoClassSummary)

set.seed(33)

comp_tree = comp_models_roc(trainset, "rpart", control,  testset[,! names(testset) %in% c("Prediction", "PredictionPR")], "green", TRUE)
plot(comp_tree,type="S", col="blue")

#comp_svm = comp_models_roc(trainset, "svmRadial", control,  testset[,! names(testset) %in% c("Prediction", "PredictionPR")], "blue", FALSE)
#plot(comp_svm,add = TRUE, col="green")
```

```{r}
comp_svm[2]
comp_tree[2]
```

```{r}
cv.values = resamples(list(svm=svm.model, rpart = rpart.model))
summary(cv.values)
```

```{r}
dotplot(cv.values, metric = "ROC") 

```

```{r}
bwplot(cv.values, layout = c(3, 1)) 

```

```{r}
splom(cv.values,metric="ROC")
```
